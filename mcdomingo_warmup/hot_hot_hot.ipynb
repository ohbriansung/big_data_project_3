{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot hot hot\n",
    "\n",
    "In this notebook, I try to figure out what the hottest recorded data point is and some info about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing\n",
    "\n",
    "First we initialize the data and load it into spark. I'm loading the smaller data set for a faster turn around time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "\n",
    "from scipy.constants import convert_temperature\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_port = \"hdfs://orion11:26990\"\n",
    "# data_path = \"/nam_s/nam_201501_s*\"\n",
    "data_path = \"/nam_s/*\"\n",
    "# data_path = \"/sample/nam_tiny*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "f = open('../features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    line = line.strip()\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line, LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line, StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line, FloatType(), True))\n",
    "        \n",
    "    \n",
    "schema = StructType(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 ms, sys: 723 Âµs, total: 2.88 ms\n",
      "Wall time: 5.15 s\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load(f'{hdfs_port}{data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "After loading in the data, I try to minimize the data set to only the relevant information: Timestamp, Geohash and temperature_surface.\n",
    "\n",
    "I then order it by the surface temperature and look at the top 30 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = df.select(\"Timestamp\", \"Geohash\", \"temperature_surface\")\n",
    "dg = dg.orderBy(F.desc(\"temperature_surface\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1440352800000, Geohash='d5f0jqerq27b', temperature_surface=330.67431640625),\n",
       " Row(Timestamp=1440266400000, Geohash='d5f0vd8eb80p', temperature_surface=330.640625),\n",
       " Row(Timestamp=1430157600000, Geohash='9g77js659k20', temperature_surface=330.6044921875),\n",
       " Row(Timestamp=1439056800000, Geohash='d5f0jqerq27b', temperature_surface=330.53662109375),\n",
       " Row(Timestamp=1440612000000, Geohash='d59d5yttuc5b', temperature_surface=330.48193359375),\n",
       " Row(Timestamp=1440612000000, Geohash='d59eqv7e03pb', temperature_surface=330.35693359375),\n",
       " Row(Timestamp=1440612000000, Geohash='d59dntd726gz', temperature_surface=330.23193359375),\n",
       " Row(Timestamp=1440698400000, Geohash='d59eqv7e03pb', temperature_surface=330.220703125),\n",
       " Row(Timestamp=1438279200000, Geohash='d5f04xyhucez', temperature_surface=330.179931640625),\n",
       " Row(Timestamp=1439488800000, Geohash='d5dpds10m55b', temperature_surface=330.14990234375),\n",
       " Row(Timestamp=1440266400000, Geohash='d59sxjhg1uh0', temperature_surface=330.140625),\n",
       " Row(Timestamp=1438279200000, Geohash='d5dpds10m55b', temperature_surface=330.054931640625),\n",
       " Row(Timestamp=1438279200000, Geohash='d5f0fgg1kg5b', temperature_surface=330.054931640625),\n",
       " Row(Timestamp=1440871200000, Geohash='d59d5yttuc5b', temperature_surface=329.9990234375),\n",
       " Row(Timestamp=1430157600000, Geohash='9g7e0htfnepz', temperature_surface=329.9794921875),\n",
       " Row(Timestamp=1439056800000, Geohash='d59eknqv867b', temperature_surface=329.91162109375),\n",
       " Row(Timestamp=1439488800000, Geohash='d59eqv7e03pb', temperature_surface=329.89990234375),\n",
       " Row(Timestamp=1439143200000, Geohash='d59d5yttuc5b', temperature_surface=329.89697265625),\n",
       " Row(Timestamp=1440266400000, Geohash='d59sh673de00', temperature_surface=329.890625),\n",
       " Row(Timestamp=1439402400000, Geohash='d5f04xyhucez', temperature_surface=329.866455078125),\n",
       " Row(Timestamp=1440612000000, Geohash='d5f0jqerq27b', temperature_surface=329.85693359375),\n",
       " Row(Timestamp=1430157600000, Geohash='9gq294h87820', temperature_surface=329.8544921875),\n",
       " Row(Timestamp=1440871200000, Geohash='d59dc7te84k0', temperature_surface=329.7490234375),\n",
       " Row(Timestamp=1440612000000, Geohash='d59d1r9p8y7b', temperature_surface=329.73193359375),\n",
       " Row(Timestamp=1440612000000, Geohash='d59eknqv867b', temperature_surface=329.73193359375),\n",
       " Row(Timestamp=1439056800000, Geohash='d5f0fgg1kg5b', temperature_surface=329.66162109375),\n",
       " Row(Timestamp=1439056800000, Geohash='d59sp10pj15b', temperature_surface=329.66162109375),\n",
       " Row(Timestamp=1439143200000, Geohash='d596q8u3hf7z', temperature_surface=329.64697265625),\n",
       " Row(Timestamp=1440266400000, Geohash='d596yu8uczh0', temperature_surface=329.640625),\n",
       " Row(Timestamp=1439661600000, Geohash='d59d5yttuc5b', temperature_surface=329.632080078125)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dg_head = dg.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Pandas\n",
    "\n",
    "Now I convert the smaller data set to a pandas data frame for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rdd = sc.parallelize(dg_head)\n",
    "p_df = rdd.toDF().toPandas()\n",
    "p_df[\"date\"] = p_df.apply(lambda x: datetime.utcfromtimestamp(x[\"Timestamp\"]//1000).strftime('%Y-%m-%d'), axis=1)\n",
    "p_df[\"fernheit\"] = p_df.apply(lambda x: convert_temperature(x.temperature_surface, 'K', 'F'), axis=1)\n",
    "p_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "It appears that while this is an anomaly (temparatures of 135F are extremely rare) it is not the only one of its kind. The rest of the top 30 values have similar surface temperatures and are all in the same \"d5\" hash region.\n",
    "\n",
    "The highest temperature recorded maps to Cancun, Mexico.\n",
    "\n",
    "![cancun](img/cancun.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
