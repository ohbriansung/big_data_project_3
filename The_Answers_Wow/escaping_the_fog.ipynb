{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "\n",
    "import geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_port = \"hdfs://orion11:26990\"\n",
    "# data_path = \"/nam_s/nam_201501_s*\"\n",
    "data_path = \"/nam_s/*\"\n",
    "# data_path = \"/sample/nam_tiny*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "f = open('../features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 ms, sys: 1.13 ms, total: 2.84 ms\n",
      "Wall time: 5.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load(f'{hdfs_port}{data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Geohashes\n",
    "\n",
    "We do a little programming to create a set of geohashes. The suffix values are a \"list\" of suffixes to attach to it's parent. For instance, each suffix in suffix_9qb will be appended to \"9qb\".  \n",
    "\n",
    "![geohashes](img/bay_area_geohashes.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9q8y', '9q8u', '9qbc', '9q9k', '9q8g', '9q9m', '9q9p', '9q8v', '9q97', '9q9h', '9q9j', '9q8x', '9qb9', '9q8z', '9qbd', '9q9n', '9q95', '9qb6', '9qb1', '9qc1', '9qbb', '9qc0', '9qbf', '9qb8'}\n"
     ]
    }
   ],
   "source": [
    "suffix_9qb = \"6df9c8b1\"\n",
    "suffix_9qc = \"10\"\n",
    "suffix_9q8 = \"xzyvug\"\n",
    "suffix_9q9 = \"pnjhmk57\"\n",
    "\n",
    "bay_area_hashes = {*[\"9qb\" + letter for letter in suffix_9qb],\n",
    "                  *[\"9qc\" + letter for letter in suffix_9qc],\n",
    "                  *[\"9q8\" + letter for letter in suffix_9q8],\n",
    "                  *[\"9q9\" + letter for letter in suffix_9q9]}\n",
    "\n",
    "print(bay_area_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "Now we do some filtering to the bay_area hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 2.47 ms, total: 14.5 ms\n",
      "Wall time: 623 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dg = df.select(\"Geohash\", \"land_cover_land1_sea0_surface\", \"visibility_surface\", df.Geohash.substr(1, 4).alias(\"front_hash\"), \"geopotential_height_cloud_base\", \"geopotential_height_surface\")\n",
    "\n",
    "dg = dg[dg.front_hash.isin(*bay_area_hashes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Command\n",
    "\n",
    "Now I check for if the clouds are lower then the land height and if that results in a decrese in visibility. I also make sure I'm looking for land locations.\n",
    "\n",
    "I count the number of foggy days vs the total number of data points for that location and that value, \"foggy_ratio\", is the percentage of times that it's foggy in that area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT \n",
      "    *, \n",
      "    foggy_days/counts AS foggy_ratio \n",
      "    FROM(\n",
      "        SELECT substr(Geohash, 1, 5) AS geoloc,\n",
      "            AVG(visibility_surface) AS vis_surf_avg,\n",
      "            SUM(CASE WHEN geopotential_height_cloud_base <= geopotential_height_surface AND visibility_surface < 24221 THEN 1 ELSE 0 END) AS foggy_days,\n",
      "            SUM(1) as counts\n",
      "        FROM nam_small \n",
      "        WHERE land_cover_land1_sea0_surface=1\n",
      "        GROUP BY substr(Geohash, 1, 5)) AS t2\n",
      "ORDER BY foggy_ratio ASC, vis_surf_avg DESC, foggy_days ASC, counts ASC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dg.createOrReplaceTempView(\"nam_small\")\n",
    "\n",
    "sql_query =f'''\n",
    "SELECT \n",
    "    *, \n",
    "    foggy_days/counts AS foggy_ratio \n",
    "    FROM(\n",
    "        SELECT substr(Geohash, 1, 5) AS geoloc,\n",
    "            AVG(visibility_surface) AS vis_surf_avg,\n",
    "            SUM(CASE WHEN geopotential_height_cloud_base <= geopotential_height_surface AND visibility_surface < 24221 THEN 1 ELSE 0 END) AS foggy_days,\n",
    "            SUM(1) as counts\n",
    "        FROM nam_small \n",
    "        WHERE land_cover_land1_sea0_surface=1\n",
    "        GROUP BY substr(Geohash, 1, 5)) AS t2\n",
    "ORDER BY foggy_ratio ASC, vis_surf_avg DESC, foggy_days ASC, counts ASC\n",
    "'''\n",
    "\n",
    "visibilities = spark.sql(sql_query)\n",
    "\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that I have the data binned up, I can convert it to a pandas data frame and convert the geohash values to longitude and latitudes and plot those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 8.45 ms, total: 34.5 ms\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p_df = visibilities.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lat(geohash):\n",
    "    return geohash2.decode(geohash)[0]\n",
    "\n",
    "def to_lon(geohash):\n",
    "    return geohash2.decode(geohash)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoloc</th>\n",
       "      <th>vis_surf_avg</th>\n",
       "      <th>foggy_days</th>\n",
       "      <th>counts</th>\n",
       "      <th>foggy_ratio</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9qc0v</td>\n",
       "      <td>23701.345179</td>\n",
       "      <td>25</td>\n",
       "      <td>404</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>38.1</td>\n",
       "      <td>-122.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9q97d</td>\n",
       "      <td>23233.363643</td>\n",
       "      <td>30</td>\n",
       "      <td>422</td>\n",
       "      <td>0.071090</td>\n",
       "      <td>37.2</td>\n",
       "      <td>-121.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9q9hp</td>\n",
       "      <td>23440.124063</td>\n",
       "      <td>31</td>\n",
       "      <td>423</td>\n",
       "      <td>0.073286</td>\n",
       "      <td>37.3</td>\n",
       "      <td>-122.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9qc0q</td>\n",
       "      <td>24002.255109</td>\n",
       "      <td>32</td>\n",
       "      <td>430</td>\n",
       "      <td>0.074419</td>\n",
       "      <td>38.</td>\n",
       "      <td>-122.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9q95r</td>\n",
       "      <td>23527.074557</td>\n",
       "      <td>33</td>\n",
       "      <td>438</td>\n",
       "      <td>0.075342</td>\n",
       "      <td>37.2</td>\n",
       "      <td>-122.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9q9nk</td>\n",
       "      <td>23079.967669</td>\n",
       "      <td>33</td>\n",
       "      <td>409</td>\n",
       "      <td>0.080685</td>\n",
       "      <td>37.7</td>\n",
       "      <td>-122.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9q9ku</td>\n",
       "      <td>21960.097769</td>\n",
       "      <td>32</td>\n",
       "      <td>394</td>\n",
       "      <td>0.081218</td>\n",
       "      <td>37.4</td>\n",
       "      <td>-121.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9q9jv</td>\n",
       "      <td>23745.016297</td>\n",
       "      <td>35</td>\n",
       "      <td>425</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>37.6</td>\n",
       "      <td>-122.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9q9nx</td>\n",
       "      <td>22922.526431</td>\n",
       "      <td>34</td>\n",
       "      <td>412</td>\n",
       "      <td>0.082524</td>\n",
       "      <td>37.7</td>\n",
       "      <td>-122.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9q9jn</td>\n",
       "      <td>23637.438335</td>\n",
       "      <td>35</td>\n",
       "      <td>420</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>37.5</td>\n",
       "      <td>-122.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geoloc  vis_surf_avg  foggy_days  counts  foggy_ratio   lat     lon\n",
       "0  9qc0v  23701.345179          25     404     0.061881  38.1  -122.1\n",
       "1  9q97d  23233.363643          30     422     0.071090  37.2  -121.9\n",
       "2  9q9hp  23440.124063          31     423     0.073286  37.3   -122.\n",
       "3  9qc0q  24002.255109          32     430     0.074419   38.  -122.1\n",
       "4  9q95r  23527.074557          33     438     0.075342  37.2   -122.\n",
       "5  9q9nk  23079.967669          33     409     0.080685  37.7  -122.1\n",
       "6  9q9ku  21960.097769          32     394     0.081218  37.4  -121.8\n",
       "7  9q9jv  23745.016297          35     425     0.082353  37.6  -122.1\n",
       "8  9q9nx  22922.526431          34     412     0.082524  37.7   -122.\n",
       "9  9q9jn  23637.438335          35     420     0.083333  37.5  -122.1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df['lat'] = p_df.apply(lambda row: to_lat(row.geoloc), axis=1)\n",
    "p_df['lon'] = p_df.apply(lambda row: to_lon(row.geoloc), axis=1)\n",
    "p_df = p_df.sort_values(['foggy_ratio', 'counts'], ascending=[1, 0])\n",
    "h_10 = p_df.head(10)\n",
    "h_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can see that all of the top 10 locations are far inland and away from where the cool ocean air meets the warm land air.\n",
    "\n",
    "![result](img/res_escape_fog.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### San Francisco Coords 37.7749° N, 122.4194° W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_coords = (37.7749, -122.4194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=sf_coords, zoom_start=8)\n",
    "\n",
    "def add_marker(folium_map, row):\n",
    "    coords = geohash2.decode(row.geoloc)\n",
    "    folium.Marker([float(coords[0]), float(coords[1])], popup=f'Foggy Ratio: {row.foggy_ratio}').add_to(folium_map)\n",
    "    print(coords)\n",
    "\n",
    "h_10.apply(lambda row: add_marker(m, row), axis=1)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
