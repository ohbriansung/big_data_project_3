{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geohash2 as gh\n",
    "import gmaps\n",
    "import gmaps.datasets\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "f = open('../features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.4 ms, sys: 11.2 ms, total: 54.6 ms\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:13030/nam_s')\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oddly Snowy\n",
    "\n",
    "What we are looking for here is snow cover where there is not very much snow coverage. For this problem we can limit ourself to only looking for areas with a snow covered surface where the surrounding areas do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|snow_cover_surface|\n",
      "+-------+------------------+\n",
      "|  count|         108000000|\n",
      "|   mean|16.332520101851852|\n",
      "| stddev| 36.90268917507707|\n",
      "|    min|               0.0|\n",
      "|    max|             100.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe([\n",
    "    'snow_cover_surface',\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "We will take a broad area and find areas in which there are little snow. Areas with only a few areas with snow can be considered strangly snowy. In this case we only group by the Geospace and do not group by the Times. After we find the general area where there is a strangly snowy location we will narrow it down to a more specific geohash in a sepreate query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Locations = 10\n",
      "CPU times: user 43.9 ms, sys: 13.7 ms, total: 57.6 ms\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.createOrReplaceTempView(\"nam_small\")\n",
    "snowy_location = spark.sql(\n",
    "    f'''SELECT substr(Geohash,1,4) as geoloc,count(substr(Geohash,1,4)) as count\n",
    "        FROM nam_small \n",
    "        WHERE snow_cover_surface > 0\n",
    "        GROUP BY substr(Geohash,1,4)\n",
    "        ORDER BY count \n",
    "        limit 10\n",
    "    ''').collect()\n",
    "print(f'Number of Locations = {len(snowy_location)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the geohashs that have a strangly snowy area/location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(geoloc='9v11', count=1)\n",
      "Row(geoloc='9udw', count=1)\n",
      "Row(geoloc='9qc0', count=1)\n",
      "Row(geoloc='9ubn', count=1)\n",
      "Row(geoloc='9sqc', count=1)\n",
      "Row(geoloc='9sxh', count=1)\n",
      "Row(geoloc='9sx0', count=1)\n",
      "Row(geoloc='9ucq', count=1)\n",
      "Row(geoloc='9ubr', count=1)\n",
      "Row(geoloc='9q96', count=1)\n"
     ]
    }
   ],
   "source": [
    "for row in snowy_location:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334b2623a9bb415392e9972c7a84121f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(layout=FigureLayout(height='420px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lat_long_list = [gh.decode(row[0]) for row in snowy_location]\n",
    "lat_long_list = [(float(row[0]), float(row[1])) for row in lat_long_list]\n",
    "\n",
    "lighting = gmaps.symbol_layer(\n",
    "    lat_long_list,\n",
    "    fill_color='red',\n",
    "    stroke_color='red',\n",
    ")\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(lighting)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code finds a the specific geo hash of the strangly snowy location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Locations = 1\n",
      "CPU times: user 69.2 ms, sys: 25 ms, total: 94.2 ms\n",
      "Wall time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.createOrReplaceTempView(\"nam_small\")\n",
    "snowy_location = spark.sql(\n",
    "    f'''SELECT Geohash, snow_cover_surface\n",
    "        FROM nam_small \n",
    "        WHERE substr(Geohash,1,4) = '9sxh' and snow_cover_surface = 100.0\n",
    "    ''').collect()\n",
    "print(f'Number of Locations = {len(snowy_location)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Geohash='9sxhw0ef52zz', snow_cover_surface=100.0)]\n"
     ]
    }
   ],
   "source": [
    "print(snowy_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cf79d30650425dacac43bcabd3b480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(layout=FigureLayout(height='420px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lat_long_list = [gh.decode(row[0]) for row in snowy_location]\n",
    "lat_long_list = [(float(row[0]), float(row[1])) for row in lat_long_list]\n",
    "\n",
    "lighting = gmaps.symbol_layer(\n",
    "    lat_long_list,\n",
    "    fill_color='red',\n",
    "    stroke_color='red',\n",
    ")\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(lighting)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Locations = 1\n",
      "CPU times: user 43.1 ms, sys: 14.3 ms, total: 57.4 ms\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.createOrReplaceTempView(\"nam_small\")\n",
    "snowy_location = spark.sql(\n",
    "    f'''SELECT Geohash, snow_cover_surface\n",
    "        FROM nam_small \n",
    "        WHERE substr(Geohash,1,4) = '9sqc' and snow_cover_surface = 100.0\n",
    "    ''').collect()\n",
    "print(f'Number of Locations = {len(snowy_location)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
